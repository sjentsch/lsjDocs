# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, Danielle J. Navarro & David R. Foxcroft. This work is
# licensed under a Creative Commons Attribution-Non Commercial 4.0
# International License.
# This file is distributed under the same license as the Learning statistics
# with jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Learning statistics with jamovi \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-01-18 19:57+0100\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../../lsj/Ch12_Regression_04.rst:4
msgid "Estimating a linear regression model"
msgstr ""

#: ../../lsj/Ch12_Regression_04.rst:12
msgid ""
"Depiction of the residuals associated with the best fitting regression "
"line (left panel), and the residuals associated with a poor regression "
"line (right panel). The residuals are much smaller for the good "
"regression line. Again, this is no surprise given that the good line is "
"the one that goes right through the middle of the data."
msgstr ""

#: ../../lsj/Ch12_Regression_04.rst:20
msgid ""
"Okay, now let’s redraw our pictures but this time I’ll add some lines to "
"show the size of the residual for all observations. When the regression "
"line is good, our residuals (the lengths of the solid black lines) all "
"look pretty small, as shown in :numref:`fig-regression3` (left panel), "
"but when the regression line is a bad one the residuals are a lot larger,"
" as you can see from looking at :numref:`fig-regression3` (right panel). "
"Hmm. Maybe what we “want” in a regression model is *small* residuals. "
"Yes, that does seem to make sense. In fact, I think I’ll go so far as to "
"say that the “best fitting” regression line is the one that has the "
"smallest residuals. Or, better yet, since statisticians seem to like to "
"take squares of everything why not say that: The estimated regression "
"coefficients, :math:`\\hat{b}_0` and :math:`\\hat{b}_1`, are those that "
"minimise the sum of the squared residuals, which we could either write as"
msgstr ""

#: ../../lsj/Ch12_Regression_04.rst:34
msgid ""
"\\sum_i (Y_i - \\hat{Y}_i)^2\n"
"\n"
msgstr ""

#: ../../lsj/Ch12_Regression_04.rst:36
msgid "or as"
msgstr ""

#: ../../lsj/Ch12_Regression_04.rst:38
msgid ""
"\\sum_i \\epsilon_{i}^2\n"
"\n"
msgstr ""

#: ../../lsj/Ch12_Regression_04.rst:40
msgid ""
"Yes, yes that sounds even better. And since I’ve indented it like that, "
"it probably means that this is the right answer. And since this is the "
"right answer, it’s probably worth making a note of the fact that our "
"regression coefficients are *estimates* (we’re trying to guess the "
"parameters that describe a population!), which is why I’ve added the "
"little hats, so that we get :math:`\\hat{b}_0` and :math:`\\hat{b}_1` "
"rather than *b*\\ :sub:`0` and *b*\\ :sub:`1`. Finally, I should also "
"note that, since there’s actually more than one way to estimate a "
"regression model, the more technical name for this estimation process is "
"**ordinary least squares (OLS) regression**."
msgstr ""

#: ../../lsj/Ch12_Regression_04.rst:51
msgid ""
"At this point, we now have a concrete definition for what counts as our "
"“best” choice of regression coefficients, :math:`\\hat{b}_0` and "
":math:`\\hat{b}_1`. The natural question to ask next is, if our optimal "
"regression coefficients are those that minimise the sum squared "
"residuals, how do we *find* these wonderful numbers? The actual answer to"
" this question is complicated and doesn’t help you understand the logic "
"of regression.\\ [#]_ This time I’m going to let you off the hook. "
"Instead of showing you the long and tedious way first and then "
"“revealing” the wonderful shortcut that jamovi provides, let’s cut "
"straight to the chase and just use jamovi to do all the heavy lifting."
msgstr ""

#: ../../lsj/Ch12_Regression_04.rst:63
msgid "Linear regression in jamovi"
msgstr ""

#: ../../lsj/Ch12_Regression_04.rst:71
msgid "jamovi screenshot showing a simple linear regression analysis"
msgstr ""

#: ../../lsj/Ch12_Regression_04.rst:75
msgid ""
"To run my linear regression, open up the ``Regression`` - ``Linear "
"Regression`` analysis in jamovi, using the ``parenthood`` dataset. Then "
"specify ``dan.grump`` as the ``Dependent Variable`` and ``dan.sleep`` as "
"the variable entered in the ``Covariates`` box. This gives the results "
"shown in :numref:`fig-reg1`, showing an intercept :math:`\\hat{b}_0` = "
"125.96 and the slope :math:`\\hat{b}_1` = -8.94. In other words, the "
"best-fitting regression line that I plotted in :numref:`fig-regression1` "
"has this formula:"
msgstr ""

#: ../../lsj/Ch12_Regression_04.rst:83
msgid "*Ŷ*\\ :sub:`i` = 125.96 + (-8.94 \\ *X*\\ :sub:`i`)"
msgstr ""

#: ../../lsj/Ch12_Regression_04.rst:86
msgid "Interpreting the estimated model"
msgstr ""

#: ../../lsj/Ch12_Regression_04.rst:88
msgid ""
"The most important thing to be able to understand is how to interpret "
"these coefficients. Let’s start with :math:`\\hat{b}_1`, the slope. If we"
" remember the definition of the slope, a regression coefficient of "
":math:`\\hat{b}_1` = -8.94 means that if I increase *X*\\ :sub:`i` by 1, "
"then I’m decreasing *Y*\\ :sub:`i` by 8.94. That is, each additional hour"
" of sleep that I gain will improve my mood, reducing my grumpiness by "
"8.94 grumpiness points. What about the intercept? Well, since "
":math:`\\hat{b}_0` corresponds to “the expected value of *Y*\\ :sub:`i` "
"when *X*\\ :sub:`i` equals 0”, it’s pretty straightforward. It implies "
"that if I get zero hours of sleep (*X*\\ :sub:`i` = 0) then my grumpiness"
" will go off the scale, to an insane value of (*Y*\\ :sub:`i` = "
"\\125.96). Best to be avoided, I think."
msgstr ""

#: ../../lsj/Ch12_Regression_04.rst:103
msgid ""
"Or at least, I’m assuming that it doesn’t help most people. But on the "
"off-chance that someone reading this is a proper kung fu master of linear"
" algebra (and to be fair, I always have a few of these people in my "
"intorductory statistics class), it *will* help *you* to know that the "
"solution to the estimation problem turns out to be :math:`\\hat{b} = "
"(\\mathbf{X}^\\prime\\mathbf{X})^{-1} \\mathbf{X}^\\prime y`, where "
":math:`\\hat{b}` is a vector containing the estimated regression "
"coefficients, **X** is the “design matrix” that contains the predictor "
"variables (plus an additional column containing all ones; strictly **X** "
"is a matrix of the regressors, but I haven’t discussed the distinction "
"yet), and *y* is a vector containing the outcome variable. For everyone "
"else, this isn’t exactly helpful and can be downright scary. However, "
"since quite a few things in linear regression can be written in linear "
"algebra terms, you’ll see a bunch of footnotes like this one in this "
"chapter. If you can follow the maths in them, great. If not, ignore it."
msgstr ""

