# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, Danielle J. Navarro & David R. Foxcroft. This work is
# licensed under a Creative Commons Attribution-Non Commercial 4.0
# International License.
# This file is distributed under the same license as the Learning statistics
# with jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Learning statistics with jamovi \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-01-18 19:57+0100\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../../lsj/Ch12_Regression_01.rst:4
msgid "Correlations"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:6
msgid ""
"In this section we’ll talk about how to describe the relationships "
"*between* variables in the data. To do that, we want to talk mostly about"
" the **correlation** between variables. But first, we need some data."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:12
msgid "The data"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:15
msgid "Descriptive statistics for the parenthood data."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:18
msgid "variable"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:18
msgid "min"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:18
msgid "max"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:18
msgid "mean"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:18
msgid "median"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:18
msgid "std. dev"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:18
msgid "IQR"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:20
msgid "Dan’s grumpiness"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:20
msgid "41"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:20
msgid "91"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:20
msgid "63.71"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:20
msgid "62"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:20
msgid "10.05"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:20
msgid "14"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:21
msgid "Dan’s hours slept"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:21
msgid "4.84"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:21
msgid "9.00"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:21
msgid "6.97"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:21
msgid "7.03"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:21
msgid "1.02"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:21
msgid "1.45"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:22
msgid "Dan’s son’s hours slept"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:22
msgid "3.25"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:22
msgid "12.07"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:22
msgid "8.05"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:22
msgid "7.95"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:22
msgid "2.07"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:22
msgid "3.21"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:25
msgid ""
"Let’s turn to a topic close to every parent’s heart: sleep. The data set "
"we’ll use is fictitious, but based on real events. Suppose I’m curious to"
" find out how much my infant son’s sleeping habits affect my mood. Let’s "
"say that I can rate my grumpiness very precisely, on a scale from 0 (not "
"at all grumpy) to 100 (grumpy as a very, very grumpy old man or woman). "
"And lets also assume that I’ve been measuring my grumpiness, my sleeping "
"patterns and my son’s sleeping patterns for quite some time now. Let’s "
"say, for 100 days. And, being a nerd, I’ve saved the data as a file "
"called ``parenthood.csv``. If we load the data we can see that the file "
"contains four variables ``dan.sleep``, ``baby.sleep``, ``dan.grump`` and "
"``day``. Note that when you first load this data set jamovi may not have "
"guessed the data type for each variable correctly, in which case you "
"should fix it: ``dan.sleep``, ``baby.sleep``, ``dan.grump`` and ``day`` "
"can be specified as continuous variables, and ``ID`` is a "
"nominal(integer) variable.\\ [#]_"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:41
msgid ""
"Next, I’ll take a look at some basic descriptive statistics and, to give "
"a graphical depiction of what each of the three interesting variables "
"looks like, :numref:`fig-grumpHist` plots histograms. One thing to note: "
"just because jamovi can calculate dozens of different statistics doesn’t "
"mean you should report all of them. If I were writing this up for a "
"report, I’d probably pick out those statistics that are of most interest "
"to me (and to my readership), and then put them into a nice, simple table"
" like the one in :numref:`tab-parenthood`.\\ [#]_ Notice that when I put "
"it into a table, I gave everything “human readable” names. This is always"
" good practice. Notice also that I’m not getting enough sleep. This isn’t"
" good practice, but other parents tell me that it’s pretty standard."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:59
msgid ""
"Histograms for the three interesting variables in the ``parenthood`` data"
" set"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:65
msgid "The strength and direction of a relationship"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:67
msgid ""
"We can draw scatterplots to give us a general sense of how closely "
"related two variables are. Ideally though, we might want to say a bit "
"more about it than that. For instance, let’s compare the relationship "
"between ``dan.sleep`` and ``dan.grump`` (:numref:`fig-grumpCor1`, left) "
"with that between ``baby.sleep`` and ``dan.grump`` (:numref:`fig-"
"grumpCor1`, right). When looking at these two plots side by side, it’s "
"clear that the relationship is *qualitatively* the same in both cases: "
"more sleep equals less grump! However, it’s also pretty obvious that the "
"relationship between ``dan.sleep`` and ``dan.grump`` is *stronger* than "
"the relationship between ``baby.sleep`` and ``dan.grump``. The plot on "
"the left is “neater” than the one on the right. What it feels like is "
"that if you want to predict what my mood is, it’d help you a little bit "
"to know how many hours my son slept, but it’d be more helpful to know how"
" many hours I slept."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:88
msgid ""
"Scatterplots showing the relationship between ``dan.sleep`` and "
"``dan.grump`` (left panel) and the relationship between ``baby.sleep`` "
"and ``dan.grump`` (right panel)."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:94
msgid ""
"In contrast, let’s consider the two scatterplots shown in :numref:`fig-"
"grumpCor2`. If we compare the scatterplot of ``baby.sleep`` vs. "
"``dan.grump`` (left) to the scatterplot of ``baby.sleep`` vs. "
"``dan.sleep`` (right), the overall strength of the relationship is the "
"same, but the direction is different. That is, if my son sleeps more, I "
"get *more* sleep (positive relationship, right hand side), but if he "
"sleeps more then I get *less* grumpy (negative relationship, left hand "
"side)."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:109
msgid ""
"Scatterplots showing the relationship between ``baby.sleep`` and "
"``dan.grump`` (left panel) and the relationship between ``baby.sleep`` "
"and ``dan.sleep`` (right panel)."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:116
msgid "The correlation coefficient"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:118
msgid ""
"We can make these ideas a bit more explicit by introducing the idea of a "
"**correlation coefficient** (or, more specifically, Pearson’s correlation"
" coefficient), which is traditionally denoted as *r*. The correlation "
"coefficient between two variables *X* and *Y* (sometimes denoted r\\ "
":sub:`XY`), which we’ll define more precisely in the next section, is a "
"measure that varies from -1 to 1. When *r* = -1 it means that we have a "
"perfect negative relationship, and when *r* = 1 it means we have a "
"perfect positive relationship. When *r* = 0, there’s no relationship at "
"all. If you look at :numref:`fig-corr`, you can see several plots showing"
" what different correlations look like."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:134
msgid ""
"Illustration of the effect of varying the strength and direction of a "
"correlation. In the left hand column, the correlations are 0.00, 0.33, "
"0.67 and 1.00 In the right hand column, the correlations are 0.00, -0.33,"
" -0.67 and -1.00."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:141
msgid ""
"The formula for the Pearson’s correlation coefficient can be written in "
"several different ways. I think the simplest way to write down the "
"formula is to break it into two steps. Firstly, let’s introduce the idea "
"of a **covariance**. The covariance between two variables *X* and *Y* is "
"a generalisation of the notion of the variance amd is a mathematically "
"simple way of describing the relationship between two variables that "
"isn’t terribly informative to humans"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:149
msgid ""
"\\mbox{Cov}(X,Y) = \\frac{1}{N-1} \\sum_{i=1}^N \\left(X_i - \\bar{X} "
"\\right) \\left(Y_i - \\bar{Y} \\right)\n"
"\n"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:151
msgid ""
"Because we’re multiplying (i.e., taking the “product” of) a quantity that"
" depends on *X* by a quantity that depends on *Y* and then averaging,\\ "
"[#]_ you can think of the formula for the covariance as an “average cross"
" product” between *X* and *Y*."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:156
msgid ""
"The covariance has the nice property that, if *X* and *Y* are entirely "
"unrelated, then the covariance is exactly zero. If the relationship "
"between them is positive (in the sense shown in :numref:`fig-corr`) then "
"the covariance is also positive, and if the relationship is negative then"
" the covariance is also negative. In other words, the covariance captures"
" the basic qualitative idea of correlation. Unfortunately, the raw "
"magnitude of the covariance isn’t easy to interpret as it depends on the "
"units in which *X* and *Y* are expressed and, worse yet, the actual units"
" that the covariance itself is expressed in are really weird. For "
"instance, if *X* refers to the ``dan.sleep`` variable (units: hours) and "
"*Y* refers to the ``dan.grump`` variable (units: grumps), then the units "
"for their covariance are “hours × grumps”. And I have no freaking idea "
"what that would even mean."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:169
msgid ""
"The Pearson correlation coefficient *r* fixes this interpretation problem"
" by standardising the covariance, in pretty much the exact same way that "
"the *z*-score standardises a raw score, by dividing by the standard "
"deviation. However, because we have two variables that contribute to the "
"covariance, the standardisation only works if we divide by both standard "
"deviations.\\ [#]_ In other words, the correlation between *X* and *Y* "
"can be written as follows:"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:177
msgid ""
"r_{XY}  = \\frac{\\mbox{Cov}(X,Y)}{ \\hat{\\sigma}_X \\ \\hat{\\sigma}_Y}"
"\n"
"\n"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:179
msgid ""
"By standardising the covariance, not only do we keep all of the nice "
"properties of the covariance discussed earlier, but the actual values of "
"*r* are on a meaningful scale: *r* = 1 implies a perfect positive "
"relationship and *r* = -1 implies a perfect negative relationship. I’ll "
"expand a little more on this point later, in Section `Interpreting a "
"correlation <Ch12_Regression_01.html#interpreting-a-correlation>`__. But "
"before I do, let’s look at how to calculate correlations in jamovi."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:188
msgid "Calculating correlations in jamovi"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:190
msgid ""
"Calculating correlations in jamovi can be done by clicking on the "
"``Regression`` → ``Correlation Matrix`` button. Transfer all four "
"continuous variables across into the box on the right to get the output "
"in :numref:`fig-correlations`."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:201
msgid ""
"jamovi screenshot showing correlations between variables in the "
"``parenthood`` dataset"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:206
msgid "Interpreting a correlation"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:208
msgid ""
"Naturally, in real life you don’t see many correlations of 1. So how "
"should you interpret a correlation of, say, *r* = 0.4? The honest answer "
"is that it really depends on what you want to use the data for, and on "
"how strong the correlations in your field tend to be. A friend of mine in"
" engineering once argued that any correlation less than 0.95 is "
"completely useless (I think he was exaggerating, even for engineering). "
"On the other hand, there are real cases, even in psychology, where you "
"should really expect correlations that strong. For instance, one of the "
"benchmark data sets used to test theories of how people judge "
"similarities is so clean that any theory that can’t achieve a correlation"
" of at least 0.9 really isn’t deemed to be successful. However, when "
"looking for (say) elementary correlates of intelligence (e.g., inspection"
" time, response time), if you get a correlation above 0.3 you’re doing "
"very very well. In short, the interpretation of a correlation depends a "
"lot on the context. That said, the rough guide in :numref:`tab-"
"interpretcorrelations` is pretty typical."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:225
msgid ""
"A rough guide to interpreting correlations. Note that I say a *rough* "
"guide. There aren’t hard and fast rules for what counts as strong or weak"
" relationships. It depends on the context."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:230
msgid "Correlation"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:230
msgid "Strength"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:230
msgid "Direction"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:232
msgid "-1.0 to -0.9"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:232 ../../lsj/Ch12_Regression_01.rst:241
msgid "Very strong"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:232 ../../lsj/Ch12_Regression_01.rst:233
#: ../../lsj/Ch12_Regression_01.rst:234 ../../lsj/Ch12_Regression_01.rst:235
#: ../../lsj/Ch12_Regression_01.rst:236
msgid "Negative"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:233
msgid "-0.9 to -0.7"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:233 ../../lsj/Ch12_Regression_01.rst:240
msgid "Strong"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:234
msgid "-0.7 to -0.4"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:234 ../../lsj/Ch12_Regression_01.rst:239
msgid "Moderate"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:235
msgid "-0.4 to -0.2"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:235 ../../lsj/Ch12_Regression_01.rst:238
msgid "Weak"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:236
msgid "-0.2 to  0.0"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:236 ../../lsj/Ch12_Regression_01.rst:237
msgid "Negligible"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:237
msgid "0.0 to  0.2"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:237 ../../lsj/Ch12_Regression_01.rst:238
#: ../../lsj/Ch12_Regression_01.rst:239 ../../lsj/Ch12_Regression_01.rst:240
#: ../../lsj/Ch12_Regression_01.rst:241
msgid "Positive"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:238
msgid "0.2 to  0.4"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:239
msgid "0.4 to  0.7"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:240
msgid "0.7 to  0.9"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:241
msgid "0.9 to  1.0"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:251
msgid ""
"Anscombe’s quartet: All four of these data sets have a Pearson "
"correlation of *r* = 0.816, but they are qualitatively different from one"
" another."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:256
msgid ""
"However, something that can never be stressed enough is that you should "
"*always* look at the scatterplot before attaching any interpretation to "
"the data. A correlation might not mean what you think it means. The "
"classic illustration of this is “Anscombe’s Quartet” (`Anscombe, 1973 "
"<References.html#anscombe-1973>`__\\ ), a collection of four data sets. "
"Each data set has two variables, an *X* and a *Y*. For all four data sets"
" the mean value for *X* is 9 and the mean for *Y* is 7.5. The standard "
"deviations for all *X* variables are almost identical, as are those for "
"the *Y* variables. And in each case the correlation between *X* and *Y* "
"is *r* = 0.816`. You can verify this yourself, since I happen to have "
"saved it as a dataset called ``anscombe``."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:267
msgid ""
"You’d think that these four data sets would look pretty similar to one "
"another. They do not. If we draw scatterplots of *X* against *Y* for all "
"four variables, as shown in :numref:`fig-anscombe`, we see that all four "
"of these are *spectacularly* different to each other. The lesson here, "
"which so very many people seem to forget in real life, is “*always graph "
"your raw data*” (Chapter `Drawing graphs <Ch05_Graphics.html>`__)."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:275
msgid "Spearman’s rank correlations"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:277
msgid ""
"The Pearson correlation coefficient is useful for a lot of things, but it"
" does have shortcomings. One issue in particular stands out: what it "
"actually measures is the strength of the *linear* relationship between "
"two variables. In other words, what it gives you is a measure of the "
"extent to which the data all tend to fall on a single, perfectly straight"
" line. Often, this is a pretty good approximation to what we mean when we"
" say “relationship”, and so the Pearson correlation is a good thing to "
"calculate. Sometimes though, it isn’t."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:286
msgid ""
"One very common situation where the Pearson correlation isn’t quite the "
"right thing to use arises when an increase in one variable *X* really is "
"reflected in an increase in another variable *Y*, but the nature of the "
"relationship isn’t necessarily linear. An example of this might be the "
"relationship between effort and reward when studying for an exam. If you "
"put zero effort (*X*) into learning a subject then you should expect a "
"grade of 0% (*Y*). However, a little bit of effort will cause a *massive*"
" improvement. Just turning up to lectures means that you learn a fair "
"bit, and if you just turn up to classes and scribble a few things down "
"your grade might rise to 35%, all without a lot of effort. However, you "
"just don’t get the same effect at the other end of the scale. As everyone"
" knows, it takes *a lot* more effort to get a grade of 90% than it takes "
"to get a grade of 55%. What this means is that, if I’ve got data looking "
"at study effort and grades, there’s a pretty good chance that Pearson "
"correlations will be misleading."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:303
msgid ""
"To illustrate, consider the data plotted in :numref:`fig-"
"ordinalRelationship`, showing the relationship between hours worked and "
"grade received for 10 students taking some class. The curious thing about"
" this (highly fictitious) data set is that increasing your effort "
"*always* increases your grade. It might be by a lot or it might be by a "
"little, but increasing effort will never decrease your grade. If we run a"
" standard Pearson correlation, it shows a strong relationship between "
"hours worked and grade received, with a correlation coefficient of "
"**0.91**. However, this doesn’t actually capture the observation that "
"increasing hours worked *always* increases the grade. There’s a sense "
"here in which we want to be able to say that the correlation is *perfect*"
" but for a somewhat different notion of what a “relationship” is. What "
"we’re looking for is something that captures the fact that there is a "
"perfect **ordinal relationship** here. That is, if student 1 works more "
"hours than student 2, then we can guarantee that student 1 will get the "
"better grade. That’s not what a correlation of *r* = 0.91 says at all."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:326
msgid ""
"The relationship between hours worked and grade received for a toy data "
"set consisting of only 10 students (each circle corresponds to one "
"student). The dashed line through the middle shows the linear "
"relationship between the two variables. This produces a strong Pearson "
"correlation of *r* = 0.91. However, the interesting thing to note here is"
" that there’s actually a perfect monotonic relationship between the two "
"variables. In this toy example, increasing the hours worked always "
"increases the grade received, as illustrated by the solid line. This is "
"reflected in a Spearman correlation of ρ = 1.00. With such a small data "
"set, however, it’s an open question as to which version better describes "
"the actual relationship involved."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:340
msgid ""
"How should we address this? Actually, it’s really easy. If we’re looking "
"for ordinal relationships all we have to do is treat the data as if it "
"were ordinal scale! So, instead of measuring effort in terms of “hours "
"worked”, lets rank all 10 of our students in order of hours worked. That "
"is, student 1 did the least work out of anyone (2 hours) so they get the "
"lowest rank (rank = 1). Student 4 was the next laziest, putting in only 6"
" hours of work over the whole semester, so they get the next lowest rank "
"(rank = 2). Notice that I’m using “rank =1” to mean “low rank”. Sometimes"
" in everyday language we talk about “rank = 1” to mean “top rank” rather "
"than “bottom rank”. So be careful, you can rank “from smallest value to "
"largest value” (i.e., small equals rank 1) or you can rank “from largest "
"value to smallest value” (i.e., large equals rank 1). In this case, I’m "
"ranking from smallest to largest, but as it’s really easy to forget which"
" way you set things up you have to put a bit of effort into remembering!"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:356
msgid ""
"Okay, so let’s have a look at our students when we rank them from worst "
"to best in terms of effort and reward:"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:362
msgid "student 1"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:362
msgid "1"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:363
msgid "student 2"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:363
msgid "10"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:364
msgid "student 3"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:364
msgid "6"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:365
msgid "student 4"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:365
msgid "2"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:366
msgid "student 5"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:366
msgid "3"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:367
msgid "student 6"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:367
msgid "5"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:368
msgid "student 7"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:368
msgid "4"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:369
msgid "student 8"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:369
msgid "8"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:370
msgid "student 9"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:370
msgid "7"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:371
msgid "student 10"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:371
msgid "9"
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:374
msgid ""
"Hmm. These are *identical*. The student who put in the most effort got "
"the best grade, the student with the least effort got the worst grade, "
"etc. As the table above shows, these two rankings are identical, so if we"
" now correlate them we get a perfect relationship, with a correlation of "
"**1.0**."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:380
msgid ""
"What we’ve just re-invented is **Spearman’s rank order correlation**, "
"usually denoted *ρ* to distinguish it from the Pearson correlation *r*. "
"We can calculate Spearman’s ρ using jamovi simply by clicking the "
"``Spearman`` check box in the ``Correlation Matrix`` analysis panel."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:388
msgid ""
"I’ve noticed that in some versions of jamovi you can also specify an "
"``ID`` variable type, but for our purposes it does not matter how we "
"specify the ID variable as we won’t be including it in any analyses."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:393
msgid ""
"Actually, even that table is more than I’d bother with. In practice, most"
" people pick *one* measure of central tendency, and *one* measure of "
"variability only."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:398
msgid ""
"Just like we saw with the variance and the standard deviation, in "
"practice we divide by *N* - 1 rather than *N*."
msgstr ""

#: ../../lsj/Ch12_Regression_01.rst:402
msgid "This is an oversimplification, but it’ll do for our purposes."
msgstr ""

