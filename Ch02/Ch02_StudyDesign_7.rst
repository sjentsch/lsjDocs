.. sectionauthor:: `Danielle J. Navarro <https://djnavarro.net/>`_ and `David R. Foxcroft <https://www.davidfoxcroft.com/>`_

Confounders, artefacts and other threats to validity
----------------------------------------------------

If we look at the issue of validity in the most general fashion the two
biggest worries that we have are *confounders* and *artefacts*. These
two terms are defined in the following way:

-  **Confounder**: A confounder is an additional, often unmeasured
   variable\ [#]_ that turns out to be related to both the predictors and
   the outcome. The existence of confounders threatens the internal
   validity of the study because you can not tell whether the predictor
   causes the outcome, or if the confounding variable causes it.

-  **Artefact**: A result is said to be “artefactual” if it only holds
   in the special situation that you happened to test in your study. The
   possibility that your result is an artefact poses a threat to your
   external validity, because it raises the possibility that you can not
   generalise or apply your results to the actual population that you care
   about.

As a general rule confounders are a bigger concern for non-experimental
studies, precisely because they are not proper experiments. By
definition, you are leaving lots of things uncontrolled, so there is a lot
of scope for confounders being present in your study. Experimental
research tends to be much less vulnerable to confounders. The more
control you have over what happens during the study, the more you can
prevent confounders from affecting the results. With random allocation,
for example, confounders are distributed randomly, and evenly, between
different groups.

However, there are always swings and roundabouts and when we start
thinking about artefacts rather than confounders the shoe is very firmly
on the other foot. For the most part, artefactual results tend to be more
of a concern for experimental studies than for non-experimental studies.
To see this, it helps to realise that the reason that a lot of studies are
non-experimental is precisely because what the researcher is trying to
do is examine human behaviour in a more naturalistic context. By working
in a more real-world context you lose experimental control (making
yourself vulnerable to confounders), but because you tend to be studying
human psychology “in the wild” you reduce the chances of getting an
artefactual result. Or, to put it another way, when you take psychology
out of the wild and bring it into the lab (which we usually have to do
to gain our experimental control), you always run the risk of
accidentally studying something different to what you wanted to study.

Be warned though. The above is a rough guide only. It is absolutely
possible to have confounders in an experiment, and to get artefactual
results with non-experimental studies. This can happen for all sorts of
reasons, not least of which is experimenter or researcher error. In
practice, it is really hard to think everything through ahead of time and
even very good researchers make mistakes.

Although there is a sense in which almost any threat to validity can be
characterised as a confounder or an artefact, they are pretty vague
concepts. So let us have a look at some of the most common examples.

History effects
~~~~~~~~~~~~~~~

**History effects** refer to the possibility that specific events may
occur during the study that might influence the outcome measure. For
instance, something might happen in between a pre-test and a post-test.
Or in-between testing participant 23 and participant 24. Alternatively,
it might be that you are looking at a paper from an older study that was
perfectly valid for its time, but the world has changed enough since
then that the conclusions are no longer trustworthy. Examples of things
that would count as history effects are:

-  You are interested in how people think about risk and uncertainty. You
   started your data collection in December 2010. But finding
   participants and collecting data takes time, so you are still finding
   new people in February 2011. Unfortunately for you (and even more
   unfortunately for others), the Queensland floods occurred in January
   2011 causing billions of dollars of damage and killing many people.
   Not surprisingly, the people tested in February 2011 express quite
   different beliefs about handling risk than the people tested in
   December 2010. Which (if any) of these reflects the “true” beliefs of
   participants? I think the answer is probably both. The Queensland
   floods genuinely changed the beliefs of the Australian public, though
   possibly only temporarily. The key thing here is that the “history”
   of the people tested in February is quite different to people tested
   in December.

-  You are testing the psychological effects of a new anti-anxiety drug.
   So what you do is measure anxiety before administering the drug
   (e.g., by self-report, and taking physiological measures). Then you
   administer the drug, and afterwards you take the same measures. In
   the interim however, because your lab is in Los Angeles, there is an
   earthquake which increases the anxiety of the participants.

Maturation effects
~~~~~~~~~~~~~~~~~~

As with history effects, **maturational effects** are fundamentally
about change over time. However, maturation effects are not in response
to specific events. Rather, they relate to how people change on their
own over time. We get older, we get tired, we get bored, etc. Some
examples of maturation effects are:

-  When doing developmental psychology research you need to be aware
   that children grow up quite rapidly. So, suppose that you want to
   find out whether some educational trick helps with vocabulary size
   among 3-year-olds. One thing that you need to be aware of is that the
   vocabulary size of children that age is growing at an incredible rate
   (multiple words per day) all on its own. If you design your study
   without taking this maturational effect into account, then you will not
   be able to tell if your educational trick works.

-  When running a very long experiment in the lab (say, something that
   lasts for three hours) it is very likely that people will begin to get
   bored and tired, and that this maturational effect will cause
   performance to decline regardless of anything else going on in the
   experiment

Repeated testing effects
~~~~~~~~~~~~~~~~~~~~~~~~

An important type of history effect is the effect of **repeated
testing**. Suppose I want to take two measurements of some psychological
construct (e.g., anxiety). One thing I might be worried about is if the
first measurement has an effect on the second measurement. In other
words, this is a history effect in which the “event” that influences the
second measurement is the first measurement itself! This is not at all
uncommon. Examples of this include:

-  *Learning and practice*: e.g., “intelligence” at time 2 might appear
   to go up relative to time 1 because participants learned the general
   rules of how to solve “intelligence-test-style” questions during the
   first testing session.

-  *Familiarity with the testing situation*: e.g., if people are nervous
   at time 1, this might make performance go down. But after sitting
   through the first testing situation they might calm down a lot
   precisely because they have seen what the testing looks like.

-  *Auxiliary changes caused by testing*: e.g., if a questionnaire
   assessing mood is boring then mood rating at measurement time 2 is
   more likely to be “bored” precisely because of the boring measurement
   made at time 1.

Selection bias
~~~~~~~~~~~~~~

**Selection bias** is a pretty broad term. Suppose that you are running
an experiment with two groups of participants where each group gets a
different “treatment”, and you want to see if the different treatments
lead to different outcomes. However, suppose that, despite your best
efforts, you have ended up with a gender imbalance across groups (say,
group A has 80\% females and group B has 50\% females). It might sound
like this could never happen but, trust me, it can. This is an example
of a selection bias, in which the people “selected into” the two groups
have different characteristics. If any of those characteristics turns
out to be relevant (say, your treatment works better on females than
males) then you are in a lot of trouble.

Differential attrition
~~~~~~~~~~~~~~~~~~~~~~

When thinking about the effects of attrition, it is sometimes helpful to
distinguish between two different types. The first is **homogeneous
attrition**, in which the attrition effect is the same for all groups,
treatments or conditions. In the example I gave above, the attrition
would be homogeneous if (and only if) the easily bored participants are
dropping out of all of the conditions in my experiment at about the same
rate. In general, the main effect of homogeneous attrition is likely to
be that it makes your sample unrepresentative. As such, the biggest
worry that you will have is that the generalisability of the results
decreases. In other words, you lose external validity.

The second type of attrition is **heterogeneous attrition**, in which
the attrition effect is different for different groups. More often
called **differential attrition**, this is a kind of selection bias that
is caused by the study itself. Suppose that, for the first time ever in
the history of psychology, I manage to find the perfectly balanced and
representative sample of people. I start running “Dani’s incredibly long
and tedious experiment” on my perfect sample but then, because my study
is incredibly long and tedious, lots of people start dropping out. I
can not stop this. Participants absolutely have the right to stop doing
any experiment, any time, for whatever reason they feel like, and as
researchers we are morally (and professionally) obliged to remind people
that they do have this right. So, suppose that “Dani’s incredibly long
and tedious experiment” has a very high drop out rate. What do you
suppose the odds are that this drop out is random? Answer: zero. Almost
certainly the people who remain are more conscientious, more tolerant of
boredom, etc., than those that leave. To the extent that (say)
conscientiousness is relevant to the psychological phenomenon that I
care about, this attrition can decrease the validity of my results.

Here is another example. Suppose I design my experiment with two
conditions. In the “treatment” condition, the experimenter insults the
participant and then gives them a questionnaire designed to measure
obedience. In the “control” condition, the experimenter engages in a bit
of pointless chitchat and then gives them the questionnaire. Leaving
aside the questionable scientific merits and dubious ethics of such a
study, let us have a think about what might go wrong here. As a general
rule, when someone insults me to my face I tend to get much less
co-operative. So, there is a pretty good chance that a lot more people
are going to drop out of the treatment condition than the control
condition. And this drop out is not going to be random. The people most
likely to drop out would probably be the people who do not care all that
much about the importance of obediently sitting through the experiment.
Since the most bloody minded and disobedient people all left the
treatment group but not the control group, we have introduced a confound:
the people who actually took the questionnaire in the treatment group
were *already* more likely to be dutiful and obedient than the people in
the control group. In short, in this study insulting people does not make
them more obedient. It makes the more disobedient people leave the
experiment! The internal validity of this experiment is completely shot.

Non-response bias
~~~~~~~~~~~~~~~~~

**Non-response bias** is closely related to selection bias and to
differential attrition. The simplest version of the problem goes like
this. You mail out a survey to 1000 people but only 300 of them reply.
The 300 people who replied are almost certainly not a random subsample.
People who respond to surveys are systematically different to people who
do not. This introduces a problem when trying to generalise from those
300 people who replied to the population at large, since you now have a
very non-random sample. The issue of non-response bias is more general
than this, though. Among the (say) 300 people that did respond to the
survey, you might find that not everyone answers every question. If
(say) 80 people chose not to answer one of your questions, does this
introduce problems? As always, the answer is maybe. If the question that
was not answered was on the last page of the questionnaire, and those 80
surveys were returned with the last page missing, there is a good chance
that the missing data is not a big deal; probably the pages just fell
off. However, if the question that 80 people did not answer was the most
confrontational or invasive personal question in the questionnaire, then
almost certainly you have got a problem. In essence, what you are dealing
with here is what is called the problem of **missing data**. If the data
that is missing was “lost” randomly, then it is not a big problem. If
it is missing systematically, then it can be a big problem.

Regression to the mean
~~~~~~~~~~~~~~~~~~~~~~

**Regression to the mean** refers to any situation where you select data
based on an extreme value on some measure. Because the variable has
natural variation it almost certainly means that when you take a
subsequent measurement the later measurement will be less extreme than
the first one, purely by chance.

Here is an example. Suppose I am interested in whether a psychology
education has an adverse effect on very smart kids. To do this, I find
the 20 psychology I students with the best high school grades and look
at how well they are doing at university. It turns out that they are doing
a lot better than average, but they are not topping the class at
university even though they did top their classes at high school. What is
going on? The natural first thought is that this must mean that the
psychology classes must be having an adverse effect on those students.
However, while that might very well be the explanation, it is more likely
that what you are seeing is an example of “regression to the mean”. To
see how it works, let us take a moment to think about what is required to
get the best mark in a class, regardless of whether that class be at
high school or at university. When you have got a big class there are
going to be *lots* of very smart people enrolled. To get the best mark
you have to be very smart, work very hard, and be a bit lucky. The exam
has to ask just the right questions for your idiosyncratic skills, and
you have to avoid making any dumb mistakes (we all do that sometimes)
when answering them. And that is the thing, whilst intelligence and hard
work are transferable from one class to the next, luck is not. The people
who got lucky in high school will not be the same as the people who get
lucky at university. That is the very definition of “luck”. The
consequence of this is that when you select people at the very extreme
values of one measurement (the top 20 students), you are selecting for
hard work, skill and luck. But because the luck does not transfer to the
second measurement (only the skill and work), these people will all be
expected to drop a little bit when you measure them a second time (at
university). So their scores fall back a little bit, back towards
everyone else. This is regression to the mean.

Regression to the mean is surprisingly common. For instance, if two very
tall people have kids their children will tend to be taller than average
but not as tall as the parents. The reverse happens with very short
parents. Two very short parents will tend to have short children, but
nevertheless those kids will tend to be taller than the parents. It can
also be extremely subtle. For instance, there have been studies done
that suggested that people learn better from negative feedback than from
positive feedback. However, the way that people tried to show this was
to give people positive reinforcement whenever they did good, and
negative reinforcement when they did bad. And what you see is that after
the positive reinforcement people tended to do worse, but after the
negative reinforcement they tended to do better. But notice that there is
a selection bias here! When people do very well, you are selecting for
“high” values, and so you should *expect*, because of regression to the
mean, that performance on the next trial should be worse regardless of
whether reinforcement is given. Similarly, after a bad trial, people
will tend to improve all on their own. The apparent superiority of
negative feedback is an artefact caused by regression to the mean
(see :ref:`Kahneman & Tversky, 1973 <Kahneman_1973>` for a discussion).

Experimenter bias
~~~~~~~~~~~~~~~~~

**Experimenter bias** can come in multiple forms. The basic idea is that
the experimenter, despite the best of intentions, can accidentally end
up influencing the results of the experiment by subtly communicating the
“right answer” or the “desired behaviour” to the participants.
Typically, this occurs because the experimenter has special knowledge
that the participant does not, for example the right answer to the
questions being asked or knowledge of the expected pattern of
performance for the condition that the participant is in. The classic
example of this happening is the case study of “Clever Hans”, which
dates back to 1907 (:ref:`Pfungst, 1911 <Pfungst_1911>`; :ref:`Hothersall,
2004 <Hothersall_2004>`). Clever Hans was a horse that apparently was able
to read and count and perform other human like feats of intelligence. After
Clever Hans became famous, psychologists started examining his behaviour
more closely. It turned out that, not surprisingly, Hans did not know how
to do maths. Rather, Hans was responding to the human observers around him,
because the humans did know how to count and the horse had learned to change
its behaviour when people changed theirs.

The general solution to the problem of experimenter bias is to engage in
double blind studies, where neither the experimenter nor the participant
knows which condition the participant is in or knows what the desired
behaviour is. This provides a very good solution to the problem, but
it is important to recognise that it is not quite ideal, and hard to pull
off perfectly. For instance, the obvious way that I could try to
construct a double blind study is to have one of my Ph.D. students (one
who does not know anything about the experiment) run the study. That
feels like it should be enough. The only person (me) who knows all the
details (e.g., correct answers to the questions, assignments of
participants to conditions) has no interaction with the participants,
and the person who does all the talking to people (the Ph.D. student)
does not know anything. Except for the reality that the last part is very
unlikely to be true. In order for the Ph.D. student to run the study
effectively they need to have been briefed by me, the researcher. And,
as it happens, the Ph.D. student also knows me and knows a bit about my
general beliefs about people and psychology (e.g., I tend to think
humans are much smarter than psychologists give them credit for). As a
result of all this, it is almost impossible for the experimenter to avoid
knowing a little bit about what expectations I have. And even a little
bit of knowledge can have an effect. Suppose the experimenter
accidentally conveys the fact that the participants are expected to do
well in this task. Well, there is a thing called the “Pygmalion effect”,
where if you expect great things of people they will tend to rise to the
occasion. But if you expect them to fail then they will do that too. In
other words, the expectations become a self-fulfilling prophesy.

Demand characteristics and reactivity
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When talking about experimenter bias, the worry is that the experimenter’s
knowledge or desires for the experiment are communicated to the participants,
and that these can change people’s behaviour (:ref:`Rosenthal, 1966
<Rosenthal_1966>`). However, even if you manage to stop this from happening,
it is almost impossible to stop people from knowing that they are part of a
psychological study. And the mere fact of knowing that someone is watching or
studying you can have a pretty big effect on behaviour. This is generally
referred to as **reactivity** or **demand characteristics**. The basic idea is
captured by the Hawthorne effect: people alter their performance because of
the attention that the study focuses on them. The effect takes its name from
a study that took place in the “Hawthorne Works” factory outside of Chicago
(see :ref:`Adair, 1984 <Adair_1984>`). This study, from the 1920s, looked at
the effects of factory lighting on worker productivity. But, importantly,
change in worker behaviour occurred because the workers *knew* they were
being studied, rather than any effect of factory lighting.

To get a bit more specific about some of the ways in which the mere fact
of being in a study can change how people behave, it helps to think like
a social psychologist and look at some of the *roles* that people might
*adopt* during an experiment but might *not adopt* if the corresponding
events were occurring in the real world:

-  The *good participant* tries to be too helpful to the researcher. He
   or she seeks to figure out the experimenter’s hypotheses and confirm
   them.

-  The *negative participant* does the exact opposite of the good
   participant. He or she seeks to break or destroy the study or the
   hypothesis in some way.

-  The *faithful participant* is unnaturally obedient. He or she seeks
   to follow instructions perfectly, regardless of what might have
   happened in a more realistic setting.

-  The *apprehensive participant* gets nervous about being tested or
   studied, so much so that his or her behaviour becomes highly
   unnatural, or overly socially desirable.

Placebo effects
~~~~~~~~~~~~~~~

The **placebo effect** is a specific type of demand characteristic that we
worry a lot about. It refers to the situation where the mere fact of being
treated causes an improvement in outcomes. The classic example comes
from clinical trials. If you give people a completely chemically inert
drug and tell them that it is a cure for a disease, they will tend to get
better faster than people who are not treated at all. In other words, it
is people’s belief that they are being treated that causes the improved
outcomes, not the drug.

However, the current consensus in medicine is that true placebo effects
are quite rare and most of what was previously considered placebo effect
is in fact some combination of natural healing (some people just get
better on their own), regression to the mean and other quirks of study
design. Of interest to psychology is that the strongest evidence for at
least some placebo effect is in self-reported outcomes, most notably in
treatment of pain (:ref:`Hróbjartsson & Gøtzsche, 2010 <Hrobjartsson_2010>`).

Situation, measurement and sub-population effects
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In some respects, these terms are a catch-all term for “all other
threats to external validity”. They refer to the fact that the choice of
sub-population from which you draw your participants, the location,
timing and manner in which you run your study (including who collects
the data) and the tools that you use to make your measurements might all
be influencing the results. Specifically, the worry is that these things
might be influencing the results in such a way that the results will not
generalise to a wider array of people, places and measures.

Fraud, deception and self-deception
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. epigraph::

   | *It is difficult to get a man to understand something,*
   | *when his salary depends on his not understanding it.*
   
   -- Upton Sinclair

There is one final thing I feel I should mention. While reading what the
textbooks often have to say about assessing the validity of a study I
could not help but notice that they seem to make the assumption that the
researcher is honest. I find this hilarious. While the vast majority of
scientists are honest, in my experience at least, some are not.\ [#]_ Not
only that, as I mentioned earlier, scientists are not immune to belief
bias. It is easy for a researcher to end up deceiving themselves into
believing the wrong thing, and this can lead them to conduct subtly
flawed research and then hide those flaws when they write it up. So you
need to consider not only the (probably unlikely) possibility of
outright fraud, but also the (probably quite common) possibility that
the research is unintentionally “slanted”. I opened a few standard
textbooks and did not find much of a discussion of this problem, so
here is my own attempt to list a few ways in which these issues can
arise:

-  **Data fabrication**. Sometimes, people just make up the data. This
   is occasionally done with “good” intentions. For instance, the
   researcher believes that the fabricated data do reflect the truth,
   and may actually reflect “slightly cleaned up” versions of actual
   data. On other occasions, the fraud is deliberate and malicious. Some
   high-profile examples where data fabrication has been alleged or
   shown include Cyril Burt (a psychologist who is thought to have
   fabricated some of his data), Andrew Wakefield (who has been accused
   of fabricating his data connecting the MMR vaccine to autism) and
   Hwang Woo-suk (who falsified a lot of his data on stem cell
   research).

-  **Hoaxes**. Hoaxes share a lot of similarities with data fabrication,
   but they differ in the intended purpose. A hoax is often a joke, and
   many of them are intended to be (eventually) discovered. Often, the
   point of a hoax is to discredit someone or some field. There is quite
   a few well known scientific hoaxes that have occurred over the years
   (e.g., Piltdown man) and some were deliberate attempts to discredit
   particular fields of research (e.g., the Sokal affair).

-  **Data misrepresentation**. While fraud gets most of the headlines,
   it is much more common in my experience to see data being
   misrepresented. When I say this I am not referring to newspapers
   getting it wrong (which they do, almost always). I am referring to the
   fact that often the data do not actually say what the researchers
   think they say. My guess is that, almost always, this is not the
   result of deliberate dishonesty but instead is due to a lack of
   sophistication in the data analyses. For instance, think back to the
   example of Simpson’s paradox that I discussed in the beginning of
   this book. It is very common to see people present “aggregated” data
   of some kind and sometimes, when you dig deeper and find the raw data
   yourself you find that the aggregated data tell a different story to
   the disaggregated data. Alternatively, you might find that some
   aspect of the data is being hidden, because it tells an inconvenient
   story (e.g., the researcher might choose not to refer to a particular
   variable). There is a lot of variants on this, many of which are very
   hard to detect.

-  **Study “misdesign”**. Okay, this one is subtle. Basically, the issue
   here is that a researcher designs a study that has built-in flaws and
   those flaws are never reported in the paper. The data that are
   reported are completely real and are correctly analysed, but they are
   produced by a study that is actually quite wrongly put together. The
   researcher really wants to find a particular effect and so the study
   is set up in such a way as to make it “easy” to (artefactually)
   observe that effect. One sneaky way to do this, in case you are
   feeling like dabbling in a bit of fraud yourself, is to design an
   experiment in which it is obvious to the participants what they are
   “supposed” to be doing, and then let reactivity work its magic for
   you. If you want you can add all the trappings of double blind
   experimentation but it will not make a difference since the study
   materials themselves are subtly telling people what you want them to
   do. When you write up the results the fraud will not be obvious to the
   reader. What is obvious to the participant when they are in the
   experimental context is not always obvious to the person reading the
   paper. Of course, the way I have described this makes it sound like
   it is always fraud. Probably there are cases where this is done
   deliberately, but in my experience the bigger concern has been with
   unintentional misdesign. The researcher *believes* and so the study
   just happens to end up with a built in flaw, and that flaw then
   magically erases itself when the study is written up for publication.

-  **Data mining & post-hoc hypothesising**. Another way in which the authors
   of a study can more or less misrepresent the data is by engaging in what is
   referred to as “data mining” (see :ref:`Gelman & Loken, 2014 <Gelman_2014>`,
   for a broader discussion of this as part of the “garden of forking paths”
   in statistical analysis). As we will discuss later, if you keep trying to
   analyse your data in lots of different ways, you will eventually find
   something that “looks” like a real effect but is not. This is referred to
   as “data mining”. It used to be quite rare because data analysis used to
   take weeks, but now that everyone has very powerful statistical software
   on their computers it is becoming very common. Data mining per se is not
   “wrong”, but the more that you do it the bigger the risk you are
   taking. The thing that is wrong, and I suspect is very common, is
   *unacknowledged* data mining. That is, the researcher runs every
   possible analysis known to humanity, finds the one that works, and
   then pretends that this was the only analysis that they ever
   conducted. Worse yet, they often “invent” a hypothesis after looking
   at the data to cover up the data mining. To be clear. It is not wrong
   to change your beliefs after looking at the data, and to reanalyse
   your data using your new “post-hoc” hypotheses. What is wrong (and I
   suspect common) is failing to acknowledge that you have done. If you
   acknowledge that you did it then other researchers are able to take
   your behaviour into account. If you do not, then they can not. And that
   makes your behaviour deceptive. Bad!

-  **Publication bias & self-censoring**. Finally, a pervasive bias is
   “non-reporting” of negative results. This is almost impossible to
   prevent. Journals do not publish every article that is submitted to
   them. They prefer to publish articles that find “something”. So, if
   20 people run an experiment looking at whether reading *Finnegans
   Wake* causes insanity in humans, and 19 of them find that it does not,
   which one do you think is going to get published? Obviously, it is the
   one study that did find that *Finnegans Wake* causes insanity.\ [#]_
   This is an example of a *publication bias*. Since no-one ever
   published the 19 studies that did not find an effect, a naive reader
   would never know that they existed. Worse yet, most researchers
   “internalise” this bias and end up *self-censoring* their research.
   Knowing that negative results are not going to be accepted for
   publication, they never even try to report them. As a friend of mine
   says “for every experiment that you get published, you also have 10
   failures”. And she is right. The catch is, while some (maybe most) of
   those studies are failures for boring reasons (e.g. you stuffed
   something up) others might be genuine “null” results that you ought
   to acknowledge when you write up the “good” experiment. And telling
   which is which is often hard to do. A good place to start is a paper
   by :ref:`Ioannidis (2005) <Ioannidis_2005>` with the depressing title
   “Why most published research findings are false”. I would also suggest
   taking a look at work by :ref:`Kühberger et al. (2014) <Kühberger_2014>`
   presenting statistical evidence that this actually happens in psychology.

There is probably a lot more issues like this to think about, but that will
do to start with. What I really want to point out is the blindingly
obvious truth that real world science is conducted by actual humans, and
only the most gullible of people automatically assumes that everyone
else is honest and impartial. Actual scientists are not usually *that*
naive, but for some reason the world likes to pretend that we are, and
the textbooks we usually write seem to reinforce that stereotype.

------

.. [#]
   The reason why I say that it is unmeasured is that if you *have*
   measured it, then you can use some fancy statistical tricks to deal
   with the confounder. Because of the existence of these statistical
   solutions to the problem of confounders, we often refer to a
   confounder that we have measured and dealt with as a *covariate*.
   Dealing with covariates is a more advanced topic, but I thought I would
   mention it in passing since it is kind of comforting to at least know
   that this stuff exists.

.. [#]
   Some people might argue that if you are not honest then you are not a
   real scientist. Which does have some truth to it I guess, but that is
   disingenuous (look up the “No true Scotsman” fallacy). The fact is
   that there are lots of people who are employed ostensibly as
   scientists, and whose work has all of the trappings of science, but
   who are outright fraudulent. Pretending that they do not exist by
   saying that they are not scientists is just muddled thinking.

.. [#]
   Clearly, the real effect is that only insane people would even try to
   read *Finnegans Wake*.
